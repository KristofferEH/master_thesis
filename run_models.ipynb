{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file runs all the models developed in this work. The operational models are listed after the defined functions and classes. Please note that the file paths are configured to match my environment. To use the models for making predictions, you will need to update these paths to match your own setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defines all Necessary Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kristoffer/anaconda3/envs/master_thesis_environment_new/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-05 12:45:24,424\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2024-06-05 12:45:24,565\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "# Imports all the necessary packages\n",
    "import pandas as pd\n",
    "import rdkit\n",
    "import rdkit.Chem\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import random_split\n",
    "from torch_geometric.nn import global_add_pool, GCNConv\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "CB_color_cycle = ['#377eb8', '#ff7f00', '#4daf4a',\n",
    "                  '#f781bf', '#a65628', '#984ea3',\n",
    "                  '#999999', '#e41a1c', '#dede00']\n",
    "\n",
    "# My data only consists of 9 different elements. These are given in the dictionary below, along with their atom number.\n",
    "elements_allowed = {1: \"H\", 6: \"C\", 7: \"N\", 8: \"O\", 9: \"F\", 16: \"S\", 17: \"Cl\", 35: \"Br\", 53: \"I\"}\n",
    "num_of_features = 9\n",
    "best_config = {'num_gcn_layers': 4, 'num_hidden_layers': 4, 'GCN_output_per_layer': [100, 420, 140, 140], 'hidden_neurons_per_layer': [260, 60, 180, 100], 'learning_rate': 0.007425096728429009, 'size_of_batch': 128, 'number_of_epochs': 600, 'number_of_temp': 7, 'min_interval': 20, 'patience': 50, 'dropout_rate': 0.2,  'activation_function': 'relu', 'decay_rate': 0.95}\n",
    "\n",
    "\n",
    "\n",
    "def extract_training_SMILES_temperature_and_target(excel_filename, path_to_code):\n",
    "    \"\"\"\n",
    "    Extracts the data that will be used for training and validating the model. This data is found in a folder called\n",
    "    \"T=temperature\", where temperature is the temperature we want to focus on. The data is found in a file called\n",
    "    \"Development.xlsx\" in this folder, which contains the SMILES notation for all the compounds, along with the\n",
    "    corresponding vapor pressure at the given temperature. Will by default extract the data from the file\n",
    "    \"Development.xlsx\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Locates the file for training data.\n",
    "    excel_data = pd.read_excel(f\"{path_to_code}/Data/{excel_filename}\")\n",
    "\n",
    "    X_dev = []\n",
    "    y_dev = []\n",
    "    temp = []    \n",
    "\n",
    "    for _, row in excel_data.iterrows():\n",
    "        SMILES = row['SMILES']\n",
    "        for column in row.index:\n",
    "            if column == 'SMILES':\n",
    "                continue\n",
    "            \n",
    "            if column.startswith('T') and not column.startswith('TMIN') and not column.startswith('TMAX') and not column.startswith('Temperature Interval'):\n",
    "                \n",
    "                if not pd.isna(row[column]):  # Check if the value is not NaN\n",
    "                    # Extracts the temperature and the logP value from the cells. \n",
    "                    cell_value = row[column].split(\"=\")\n",
    "                    temperature = float(cell_value[0][2:-2])\n",
    "                    logP = float(cell_value[1])\n",
    "\n",
    "                    # Appends the values to the lists.\n",
    "                    X_dev.append(SMILES)\n",
    "                    y_dev.append(logP)\n",
    "                    temp.append(float(temperature))\n",
    "\n",
    "    return X_dev, y_dev, temp\n",
    "\n",
    "\n",
    "def extract_mean_std(number_of_temp, min_interval):\n",
    "    \"\"\"\n",
    "    Extracts the mean and standard deviation for the training data. \n",
    "    \"\"\"\n",
    "    with open(f\"Data/Min interval of {min_interval} ÂºC/{number_of_temp} temperatures/Pressure Overview (log).txt\", 'r') as file:\n",
    "        data = file.readlines()\n",
    "        for line in data: \n",
    "            if line.startswith(\"Mean pressure in training data\"):\n",
    "                mean = float(line.split(\":\")[1].strip())\n",
    "            if line.startswith(\"Standard deviation of pressure in training data\"):\n",
    "                std = float(line.split(\":\")[1].strip())\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def shuffle_data(X_dev, y_dev, temp):\n",
    "    \"\"\"\n",
    "    This function shuffles the data. \n",
    "    \"\"\"\n",
    "\n",
    "    # We have to shuffle the temperature and target values in the same way as the SMILES values.\n",
    "    shuffled_indices = list(range(len(X_dev)))\n",
    "    random.shuffle(shuffled_indices)\n",
    "\n",
    "    X_dev = [X_dev[i] for i in shuffled_indices]\n",
    "    y_dev = [y_dev[i] for i in shuffled_indices]\n",
    "    temp = [temp[i] for i in shuffled_indices]\n",
    "\n",
    "    return X_dev, y_dev, temp\n",
    "\n",
    "\n",
    "def set_seed(seed_value):\n",
    "    \"\"\"Set seed for reproducibility.\"\"\"\n",
    "    np.random.seed(seed_value)  # Set numpy seed\n",
    "    torch.manual_seed(seed_value)  # Set torch seed\n",
    "    random.seed(seed_value)  # Set python random seed\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # Set python environment seed\n",
    "\n",
    "\n",
    "def smiles2graph(sml):\n",
    "    \"\"\"\n",
    "    This code is based on the code from the book \"Deep Learning for Molecules and Materials\" byAndrew D White.\n",
    "    This function will return the graph of a molecule based on the SMILES string!\n",
    "    \"\"\"\n",
    "    m = rdkit.Chem.MolFromSmiles(sml)\n",
    "    m = rdkit.Chem.AddHs(m)\n",
    "    order_string = {\n",
    "        rdkit.Chem.rdchem.BondType.SINGLE: 1,\n",
    "        rdkit.Chem.rdchem.BondType.DOUBLE: 2,\n",
    "        rdkit.Chem.rdchem.BondType.TRIPLE: 3,\n",
    "        rdkit.Chem.rdchem.BondType.AROMATIC: 4,\n",
    "    }\n",
    "\n",
    "    # The length of the adjacency matrix should be NxN, where N is the number of atoms in the molecule.\n",
    "    N = len(list(m.GetAtoms()))\n",
    "    nodes = np.zeros((N, len(elements_allowed)))\n",
    "    lookup = list(elements_allowed.keys())\n",
    "    for i in m.GetAtoms():\n",
    "        # If an atom is present in our molecule,\n",
    "        nodes[i.GetIdx(), lookup.index(i.GetAtomicNum())] = 1\n",
    "\n",
    "    adj = np.zeros((N, N))\n",
    "    for j in m.GetBonds():\n",
    "        u = min(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        v = max(j.GetBeginAtomIdx(), j.GetEndAtomIdx())\n",
    "        order = j.GetBondType()\n",
    "        if order in order_string:\n",
    "            order = order_string[order]\n",
    "        else:\n",
    "            raise Warning(\"Ignoring bond order\" + order)\n",
    "        adj[u, v] = 1\n",
    "        adj[v, u] = 1\n",
    "    # We want the diagonal in the matrix to be 1.\n",
    "    adj += np.eye(N)\n",
    "    return nodes, adj\n",
    "\n",
    "\n",
    "class MolecularDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This class is needed to create our dataset (on the Dataset format).\n",
    "    The class inherits from the Dataset class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, y, temperature):\n",
    "        # Initializes the features and targets. Is out constructor.\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.temperature = temperature\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the length of the dataset\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the SMILES value of the molecule, calculate the graphs based on that SMILE, and then return this\n",
    "        # value along with the corresponding target value.\n",
    "        SMILES = self.X[idx]\n",
    "        nodes, adj = smiles2graph(SMILES)\n",
    "\n",
    "        # Convert nodes and adj to tensors, assuming they are NumPy arrays returned from smiles2graph\n",
    "        nodes_tensor = torch.tensor(nodes, dtype=torch.float32)\n",
    "        adj_tensor = torch.tensor(adj, dtype=torch.float32)\n",
    "\n",
    "        # Convert the target value to a tensor. Assuming solubility is a single floating-point value.\n",
    "        target_tensor = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "\n",
    "        # Convert the temperature to a tensor. Assuming temperature is a single floating-point value.\n",
    "        temperature_tensor = torch.tensor(self.temperature[idx], dtype=torch.float32)\n",
    "\n",
    "        return (nodes_tensor, adj_tensor), target_tensor, temperature_tensor\n",
    "\n",
    "\n",
    "def split_dataset(dataset, test_split=0.2):\n",
    "    \"\"\"\n",
    "    Splits the dataset into training and testing datasets. Will split the dataset into 80% training and 20% testing\n",
    "    if no test_split is specified.\n",
    "    \"\"\"\n",
    "    # Determine the lengths\n",
    "    test_size = int(test_split * len(dataset))\n",
    "    train_size = len(dataset) - test_size\n",
    "\n",
    "    # Sets a seed for reproducibility\n",
    "    generator = torch.Generator().manual_seed(42)\n",
    "\n",
    "    # Split the dataset. Implements a seed for reproducibility.\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size], generator=generator)\n",
    "\n",
    "    return train_dataset, test_dataset\n",
    "\n",
    "\n",
    "def merge_batch(batch):\n",
    "    \"\"\"\n",
    "    This function merge the graphs in the batch into a larger batch by concatenating node features and computing new\n",
    "    edge indices. Batch is on the format [(graph1, target1), (graph2, target2), ...], and the output is on the format\n",
    "    (merged_nodes, merged_edge_indices, merged_batch_mapping), merged_targets, merged_temperatures.\n",
    "\n",
    "    Further, merged_nodes is on the format [node1, node2, ...], merged_edge_indices is on the format [edge1, edge2, ...]\n",
    "    and merged_batch_mapping is on the format [0, 0, ..., 1, 1, ...], where the length of the lists are the number of\n",
    "    nodes and edges in the merged graph, respectively. The merged_targets is on the format [target1, target2, ...].\n",
    "    The merged_temperatures is on the format [temperature1, temperature2, ...].\n",
    "    \"\"\"\n",
    "    # Separate out nodes, adjacency matrices, scalar tensors, and temperatures\n",
    "    nodes_list = [item[0][0] for item in batch]\n",
    "    adj_list = [item[0][1] for item in batch]\n",
    "    scalar_tensors = [item[1] for item in batch]\n",
    "    temperatures = [item[2] for item in batch]\n",
    "\n",
    "    # Placeholder for the combined nodes\n",
    "    merged_nodes = []\n",
    "    # Placeholder for the combined edge indices (plural for index) -> says something about connections\n",
    "    edge_indices = []\n",
    "    # Placeholder for the batch mapping -> says something about which node that belongs to which graph! Important for\n",
    "    # tying things together.\n",
    "    batch_mapping = []\n",
    "\n",
    "    # This will keep track of the current node index offset in the combined graph -> how much we must \"shift\" the\n",
    "    # edge matrix\n",
    "    current_node_index = 0\n",
    "\n",
    "    # Iterates over all the graphs\n",
    "    for idx, (nodes, adj) in enumerate(zip(nodes_list, adj_list)):\n",
    "        # Extracts the number of nodes in the current graph.\n",
    "        num_nodes = nodes.shape[0]\n",
    "\n",
    "        # Add the current graph's nodes to the merged node list. Can append as usual, since we want to add rows.\n",
    "        merged_nodes.append(nodes)\n",
    "\n",
    "        # Converts the adjacency matrix to the correct edge index format required for the GCN layer. In other words, we\n",
    "        # find the edges in the adjacency matrix, and offset them by the current node index.\n",
    "        edges = adj.nonzero().t()\n",
    "        edges = edges + current_node_index\n",
    "\n",
    "        # Add the current graph's edges to the combined list\n",
    "        edge_indices.append(edges)\n",
    "\n",
    "        # Create the batch mapping for the current graph\n",
    "        batch_mapping.extend([idx] * num_nodes)\n",
    "\n",
    "        # Update the node index offset\n",
    "        current_node_index += num_nodes\n",
    "\n",
    "    # Convert the merged node list to a tensor\n",
    "    merged_nodes_tensor = torch.cat(merged_nodes, dim=0)\n",
    "\n",
    "    # Convert the edge index lists to a single tensor\n",
    "    merged_edge_indices_tensor = torch.cat(edge_indices, dim=1)\n",
    "\n",
    "    # Convert the batch mapping to a tensor\n",
    "    batch_mapping_tensor = torch.tensor(batch_mapping, dtype=torch.long)\n",
    "\n",
    "    # Convert scalar_tensors (target values) into a single tensor (can be converted directly).\n",
    "    merged_scalar_tensor = torch.stack(scalar_tensors)\n",
    "\n",
    "    # Convert temperatures into a single tensor.\n",
    "    merged_temperature_tensor = torch.stack(temperatures)\n",
    "\n",
    "    return (merged_nodes_tensor, merged_edge_indices_tensor, batch_mapping_tensor), merged_scalar_tensor, merged_temperature_tensor\n",
    "\n",
    "\n",
    "class AntoineEquationLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the custom layer. \n",
    "        \"\"\"\n",
    "        super(AntoineEquationLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x_in, temperature):\n",
    "        \"\"\"\n",
    "        Forward pass of the layer using the Antoine equation.\n",
    "        x_in is expected to have three elements representing A, B, and C coefficients. Note that x_in will have the\n",
    "        shape (batch_size, 3). This layer also expect a temperature as input.\n",
    "        \"\"\"\n",
    "    \n",
    "        A, B, C = x_in[:, 0], x_in[:, 1], x_in[:, 2]\n",
    "\n",
    "        # Here, P will be the pressure in mmHg! \n",
    "        log_P = A - B / (temperature + C)\n",
    "\n",
    "        return log_P, A, B, C\n",
    "\n",
    "\n",
    "class GNNModel(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the structure of the model. The model will be a graph neural network (GNN) model. The model\n",
    "    inherits from the nn.Module class, which is the base class for all neural network modules in PyTorch. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_of_features, num_gcn_layers, num_hidden_layers, num_dense_neurons, GCN_output_per_layer, \n",
    "                 dropout_rate, activation_function):\n",
    "        # Defines the structure of the model. \n",
    "        super().__init__()\n",
    "\n",
    "        # Set the activation function.\n",
    "        if activation_function == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "        elif activation_function == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "        elif activation_function == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        # Initialize GCN layers and activations as ModuleList\n",
    "        self.GCN_layers = nn.ModuleList()\n",
    "        self.GCN_activations = nn.ModuleList()\n",
    "\n",
    "        # Adds the GCN layers and the activation functions to the model.\n",
    "        for i in range(num_gcn_layers):\n",
    "            self.GCN_layers.append(GCNConv(num_of_features, GCN_output_per_layer[i]))\n",
    "            self.GCN_activations.append(self.activation)\n",
    "            num_of_features = GCN_output_per_layer[i]\n",
    "\n",
    "        # Adds the global pooling layer.\n",
    "        self.global_pooling = global_add_pool\n",
    "        self.global_pooling_activation = self.activation\n",
    "\n",
    "        # Initialize dense layers and activations as ModuleList\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        self.dense_activations = nn.ModuleList()\n",
    "        self.dropout = nn.ModuleList()\n",
    "\n",
    "        # Adds the dense layers and the activation functions to the model.\n",
    "        for i in range(num_hidden_layers):\n",
    "            self.dense_layers.append(nn.Linear(num_of_features, num_dense_neurons[i]))\n",
    "            self.dense_activations.append(self.activation)\n",
    "            self.dropout.append(nn.Dropout(p=dropout_rate))\n",
    "            num_of_features = num_dense_neurons[i]\n",
    "\n",
    "        # Adds the Antoine coefficients layer.\n",
    "        self.antonine_coeff = nn.Linear(num_of_features, 3)\n",
    "        \n",
    "        # Create the Antinone equation layer separatly as it is not possible to pass additional arguments to the\n",
    "        # Sequential class. However, we need to pass the temperature to the Antoine equation layer.\n",
    "        self.antonine_layer = AntoineEquationLayer()\n",
    "\n",
    "    def forward(self, x, edge_indices, batch_mapping, temperature, mean, std):\n",
    "        # Defines the forward pass of the model. This is where the data is input to the model.\n",
    "\n",
    "        #print(f\"Input shape: {x.shape}\")\n",
    "        # Iterates over all the GCN layers and the activation functions.\n",
    "        for layer, act in zip(self.GCN_layers, self.GCN_activations):\n",
    "            # Performs the message passing and then applies the activation function. Edge index says which atoms \n",
    "            # that are connected. \n",
    "            x = act(layer(x, edge_indices))\n",
    "            #print(f\"After GCN Layer {layer}: {x.shape}\")\n",
    "\n",
    "        # Apply global pooling. Here we need batch_mapping to map which atoms that belongs to which molecule.\n",
    "        x = self.global_pooling(x, batch_mapping)\n",
    "        x = self.global_pooling_activation(x)\n",
    "        #print(f\"After Global Pooling: {x.shape}\")\n",
    "\n",
    "        # Iterates over all the dense layers and the activation functions.\n",
    "        for layer, act, drop in zip(self.dense_layers, self.dense_activations, self.dropout):\n",
    "            # Applies the dense layer and then the activation function.\n",
    "            x = act(layer(x))\n",
    "            # Apply dropout\n",
    "            x = drop(x)\n",
    "            #print(f\"After Dense Layer {layer}: {x.shape}\")\n",
    "\n",
    "        # Apply the Antoine coefficients layer\n",
    "        x = self.antonine_coeff(x)\n",
    "        #print(f\"After Antoine Coefficients Layer: {x.shape}\")\n",
    "\n",
    "        log_P, A, B, C = self.antonine_layer(x, temperature)\n",
    "        #print(f\"Final Output Shape: {log_P.shape}\")\n",
    "\n",
    "        # Scale the output\n",
    "        log_P = (log_P - mean) / std\n",
    "\n",
    "        # Note that here I also return the Antoine coefficients. This was not done when the model was trained, but want to do it now to see \n",
    "        # the values.\n",
    "        return log_P, A, B, C\n",
    "\n",
    "\n",
    "def extract_VLE_data(excel_filename, path_to_code):\n",
    "    \"\"\"\n",
    "    Extracts the data that will be used for training and validating the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Locates the file for the training data.\n",
    "    excel_file_VLE = pd.ExcelFile(f\"{path_to_code}/Data/Combined model/{excel_filename}\")\n",
    "\n",
    "    # Locates the file for the SMILES notation.\n",
    "    smiles_converter = pd.read_excel(f\"{path_to_code}/Data/All_amines.xlsx\")\n",
    "\n",
    "    # Only want the columns \"CAS No\" and \"SMILES\"\n",
    "    smiles_converter = smiles_converter[[\"CAS No\", \"SMILES\"]]\n",
    "\n",
    "    # Dictionary to store the data. The key is the SMILES notation and the value is the VLE data.\n",
    "    dev_dict = {}\n",
    "\n",
    "    for sheet in excel_file_VLE.sheet_names:\n",
    "        # Get the correct SMILES notation for the compound.\n",
    "        smiles_value = smiles_converter.loc[smiles_converter[\"CAS No\"] == sheet, \"SMILES\"].values[0]\n",
    "        dev_dict[smiles_value] = pd.read_excel(excel_file_VLE, sheet_name=sheet)\n",
    "\n",
    "    return dev_dict\n",
    "\n",
    "\n",
    "def saturation_pressure_water(T):\n",
    "    \"\"\"\n",
    "    This function calculates the saturation pressure of water at a given temperature. These formulas are based on the\n",
    "    Antoine equation recived from the Dortmund Data Bank.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a tensor for P with the same shape as T and fill it with NaNs.\n",
    "    P = torch.full_like(T, float('nan'))\n",
    "\n",
    "    # Condition for temperature range 1 to 100 degrees Celsius\n",
    "    condition1 = (T > 1) & (T < 100)\n",
    "    A1 = 8.07131\n",
    "    B1 = 1730.63\n",
    "    C1 = 233.426\n",
    "    P[condition1] = 10**(A1 - B1 / (T[condition1] + C1))\n",
    "\n",
    "    # Condition for temperature range 100 to 374 degrees Celsius\n",
    "    condition2 = (T >= 100) & (T < 374)\n",
    "    A2 = 8.14019\n",
    "    B2 = 1810.94\n",
    "    C2 = 244.485\n",
    "    P[condition2] = 10**(A2 - B2 / (T[condition2] + C2))\n",
    "\n",
    "    return P\n",
    "\n",
    "\n",
    "class NRTLEquationLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the custom layer. \n",
    "        \"\"\"\n",
    "        super(NRTLEquationLayer, self).__init__()\n",
    "\n",
    "    def forward(self, x_in, temperature, mole_frac_amine, mole_frac_water):\n",
    "        \"\"\"\n",
    "        x_in have four elements representing alpha, b_12, and b_21 coefficients.\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the interaction parameters. \n",
    "        alpha, b_12, b_21 = x_in[:, 0], x_in[:, 1], x_in[:, 2]\n",
    "\n",
    "        # Sets alpha to be constant at 0.3 -> Better to let it vary. \n",
    "        #alpha = 0.3\n",
    "        alpha_min = 0.2\n",
    "        alpha_max = 0.47\n",
    "\n",
    "        # Scales alpha from 0 to 1\n",
    "        #alpha = torch.sigmoid(alpha)\n",
    "        # Scales alpha from [alpha_min, alpha_max]\n",
    "        #alpha = alpha_min + (alpha_max - alpha_min) * alpha\n",
    "\n",
    "        # Calculate alpha to get it in that range (from spt-nrtl paper)\n",
    "        alpha = alpha_min * (1 + torch.sigmoid(alpha)/10 * ((alpha_max/alpha_min)))\n",
    "\n",
    "        # Gas constant in mmHg\n",
    "        R = 62.36367 # mmHg L / mol K\n",
    "\n",
    "        # The temperature in Kelvin\n",
    "        temperature = temperature + 273.15\n",
    "\n",
    "        # Calculate tau \n",
    "        tau_12 = b_12/(R*temperature)\n",
    "        tau_21 = b_21/(R*temperature)\n",
    "\n",
    "        # Calculate G\n",
    "        G_12 = torch.exp(-tau_12 * alpha)\n",
    "        G_21 = torch.exp(-tau_21 * alpha)\n",
    "\n",
    "        # Calculate the activity coefficients\n",
    "        ln_gamma_amine = mole_frac_water**2 * (tau_21 * (G_21 / (mole_frac_amine + mole_frac_water * G_21) )**2 + (tau_12 * G_12)/( mole_frac_water + mole_frac_amine*G_12 )**2)\n",
    "        ln_gamma_water = mole_frac_amine**2 * (tau_12 * (G_12 / (mole_frac_water + mole_frac_amine * G_12) )**2 + (tau_21 * G_21)/( mole_frac_amine + mole_frac_water*G_21 )**2)\n",
    "\n",
    "        return ln_gamma_amine, ln_gamma_water\n",
    "\n",
    "\n",
    "class GNN_Activity_coeff(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the structure of the model. The model will be a graph neural network (GNN) model. The model\n",
    "    inherits from the nn.Module class, which is the base class for all neural network modules in PyTorch. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, num_of_features):\n",
    "        # Defines the structure of the model. \n",
    "        super().__init__()\n",
    "\n",
    "        # Set the activation function.\n",
    "        if config[\"activation_function\"] == \"relu\":\n",
    "            self.activation = nn.ReLU()\n",
    "\n",
    "        elif config[\"activation_function\"] == \"sigmoid\":\n",
    "            self.activation = nn.Sigmoid()\n",
    "\n",
    "        elif config[\"activation_function\"] == \"tanh\":\n",
    "            self.activation = nn.Tanh()\n",
    "\n",
    "        # Initialize GCN layers and activations as ModuleList\n",
    "        self.GCN_layers = nn.ModuleList()\n",
    "        self.GCN_activations = nn.ModuleList()\n",
    "\n",
    "        # Adds the GCN layers and the activation functions to the model.\n",
    "        for i in range(best_config[\"num_gcn_layers\"]):\n",
    "            self.GCN_layers.append(GCNConv(num_of_features, best_config[\"GCN_output_per_layer\"][i]))\n",
    "            self.GCN_activations.append(self.activation)\n",
    "            num_of_features = best_config[\"GCN_output_per_layer\"][i]\n",
    "\n",
    "        # Adds the global pooling layer.\n",
    "        self.global_pooling = global_add_pool\n",
    "        self.global_pooling_activation = self.activation\n",
    "\n",
    "        # Initialize dense layers and activations as ModuleList\n",
    "        self.dense_layers = nn.ModuleList()\n",
    "        self.dense_activations = nn.ModuleList()\n",
    "        self.dropout = nn.ModuleList()\n",
    "\n",
    "        # Adds the dense layers and the activation functions to the model.\n",
    "        for i in range(best_config[\"num_hidden_layers\"]):\n",
    "            self.dense_layers.append(nn.Linear(num_of_features, best_config[\"hidden_neurons_per_layer\"][i]))\n",
    "            self.dense_activations.append(self.activation)\n",
    "            self.dropout.append(nn.Dropout(p=best_config[\"dropout_rate\"]))\n",
    "            num_of_features = best_config[\"hidden_neurons_per_layer\"][i]\n",
    "\n",
    "        # Adds the NRTL coefficients layer.\n",
    "        self.NRTL_coeff = nn.Linear(num_of_features, 3)\n",
    "        \n",
    "        # Initilize the NRTRL layer\n",
    "        self.nrtl_layer = NRTLEquationLayer()\n",
    "\n",
    "    def forward(self, x, edge_indices, batch_mapping, temperature, mean, std, mole_frac_amine, mole_frac_water):\n",
    "        # Defines the forward pass of the model. This is where the data is input to the model.\n",
    "\n",
    "        #print(f\"Input shape: {x.shape}\")\n",
    "        # Iterates over all the GCN layers and the activation functions.\n",
    "        for layer, act in zip(self.GCN_layers, self.GCN_activations):\n",
    "            # Performs the message passing and then applies the activation function. Edge index says which atoms \n",
    "            # that are connected. \n",
    "            x = act(layer(x, edge_indices))\n",
    "            #print(f\"After GCN Layer {layer}: {x.shape}\")\n",
    "\n",
    "        # Apply global pooling. Here we need batch_mapping to map which atoms that belongs to which molecule.\n",
    "        x = self.global_pooling(x, batch_mapping)\n",
    "        x = self.global_pooling_activation(x)\n",
    "        #print(f\"After Global Pooling: {x.shape}\")\n",
    "\n",
    "        # Iterates over all the dense layers and the activation functions.\n",
    "        for layer, act, drop in zip(self.dense_layers, self.dense_activations, self.dropout):\n",
    "            # Applies the dense layer and then the activation function.\n",
    "            x = act(layer(x))\n",
    "            # Apply dropout\n",
    "            x = drop(x)\n",
    "            #print(f\"After Dense Layer {layer}: {x.shape}\")\n",
    "\n",
    "        # Apply the Antoine coefficients layer\n",
    "        x = self.NRTL_coeff(x)\n",
    "        #print(f\"After NRTL Coefficients Layer: {x.shape}\")\n",
    "\n",
    "        ln_gamma_amine, ln_gamma_water = self.nrtl_layer(x, temperature, mole_frac_amine, mole_frac_water)\n",
    "        #print(f\"Final Output Shape: {log_P.shape}\")\n",
    "\n",
    "        return ln_gamma_amine, ln_gamma_water\n",
    "\n",
    "\n",
    "class VLE_model(nn.Module):\n",
    "    \"\"\"\n",
    "    This class defines the structure of the model. The model will be a graph neural network (GNN) model. The model\n",
    "    inherits from the nn.Module class, which is the base class for all neural network modules in PyTorch. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, GCN_saturation, mean_gcn, std_gcn, best_config, num_of_features):\n",
    "        # Defines the structure of the model. \n",
    "        super().__init__()\n",
    "\n",
    "        # Initialize the GCN saturation model with the pre-trained model. Also initilize the mean and standard deviation for scaling.\n",
    "        self.GCN_saturation = GCN_saturation\n",
    "        self.mean_gcn = mean_gcn\n",
    "        self.std_gcn = std_gcn\n",
    "\n",
    "        # Initilize the NRTL model\n",
    "        self.NRTL_model = GNN_Activity_coeff(best_config, num_of_features)\n",
    "\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\" Defines the forward pass of the model. This is where the data is input to the model. \"\"\"\n",
    "\n",
    "        # Unpack the batch\n",
    "        (x, edge_indices, batch_mapping), x_water, x_amine, y_water, y_amine, temperature, P_tot = batch\n",
    "\n",
    "        # Calculate the predictions of the GCN model\n",
    "        log_P_normalized = self.GCN_saturation(x, edge_indices, batch_mapping, temperature, self.mean_gcn, self.std_gcn)\n",
    "        # Only the first element as the others are the antoine coefficents\n",
    "        log_P_normalized = log_P_normalized[0]\n",
    "        log_P = log_P_normalized * self.std_gcn + self.mean_gcn\n",
    "        P_amine = 10**log_P\n",
    "        ln_P_amine = torch.log(P_amine)\n",
    "\n",
    "        # Convert total pressure from Pa to mmHg\n",
    "        P_tot = P_tot / 133.322\n",
    "\n",
    "        # Take the natural logarithm of the total pressure, water fraction and amine fraction\n",
    "        ln_P_tot = torch.log(P_tot)\n",
    "        ln_x_water = torch.log(x_water)\n",
    "        ln_x_amine = torch.log(x_amine)\n",
    "\n",
    "        # Calculate the predictions of the NRTL model (activity coefficients)\n",
    "        ln_gamma_amine, ln_gamma_water = self.NRTL_model(x, edge_indices, batch_mapping, temperature, self.mean_gcn, self.std_gcn, x_amine, x_water)\n",
    "\n",
    "        # Calculate the saturation pressure of water\n",
    "        P_water = saturation_pressure_water(temperature)\n",
    "        ln_P_water = torch.log(P_water)\n",
    "\n",
    "        # Calculate ln_y_amine and ln_y_water\n",
    "        ln_y_amine_pred = ln_x_amine + ln_gamma_amine + ln_P_amine - ln_P_tot\n",
    "        ln_y_water_pred = ln_x_water + ln_gamma_water + ln_P_water - ln_P_tot\n",
    "\n",
    "        # Want to return the predictions of the amine and water fractions. \n",
    "        sum_ln_y = ln_y_amine_pred + ln_y_water_pred\n",
    "\n",
    "        return torch.stack((ln_y_amine_pred, ln_y_water_pred, sum_ln_y), dim=1)\n",
    "\n",
    "\n",
    "def merge_batch_VLE(batch):\n",
    "    \"\"\"\n",
    "    This function merge the graphs in the batch into a larger batch by concatenating node features and computing new\n",
    "    edge indices. Batch is on the format [(graph1, target1), (graph2, target2), ...], and the output is on the format\n",
    "    (merged_nodes, merged_edge_indices, merged_batch_mapping), merged_targets, merged_temperatures.\n",
    "\n",
    "    Further, merged_nodes is on the format [node1, node2, ...], merged_edge_indices is on the format [edge1, edge2, ...]\n",
    "    and merged_batch_mapping is on the format [0, 0, ..., 1, 1, ...], where the length of the lists are the number of\n",
    "    nodes and edges in the merged graph, respectively. The merged_targets is on the format [target1, target2, ...].\n",
    "    The merged_temperatures is on the format [temperature1, temperature2, ...].\n",
    "    \"\"\"\n",
    "\n",
    "    # Separate out nodes, adjacency matrices, scalar tensors, and temperatures\n",
    "    nodes_list = [item[0][0] for item in batch]\n",
    "    adj_list = [item[0][1] for item in batch]\n",
    "    x_water_tensor = [item[1] for item in batch]\n",
    "    x_amine_tensor = [item[2] for item in batch]\n",
    "    y_water_tensor = [item[3] for item in batch]\n",
    "    y_amine_tensor = [item[4] for item in batch]\n",
    "    temperatures = [item[5] for item in batch]\n",
    "    P_tot = [item[6] for item in batch]\n",
    "\n",
    "    # Placeholder for the combined nodes\n",
    "    merged_nodes = []\n",
    "    # Placeholder for the combined edge indices (plural for index) -> says something about connections\n",
    "    edge_indices = []\n",
    "    # Placeholder for the batch mapping -> says something about which node that belongs to which graph! Important for\n",
    "    # tying things together.\n",
    "    batch_mapping = []\n",
    "\n",
    "    # This will keep track of the current node index offset in the combined graph -> how much we must \"shift\" the\n",
    "    # edge matrix\n",
    "    current_node_index = 0\n",
    "\n",
    "    # Iterates over all the graphs\n",
    "    for idx, (nodes, adj) in enumerate(zip(nodes_list, adj_list)):\n",
    "        # Extracts the number of nodes in the current graph.\n",
    "        num_nodes = nodes.shape[0]\n",
    "\n",
    "        # Add the current graph's nodes to the merged node list. Can append as usual, since we want to add rows.\n",
    "        merged_nodes.append(nodes)\n",
    "\n",
    "        # Converts the adjacency matrix to the correct edge index format required for the GCN layer. In other words, we\n",
    "        # find the edges in the adjacency matrix, and offset them by the current node index.\n",
    "        edges = adj.nonzero().t()\n",
    "        edges = edges + current_node_index\n",
    "\n",
    "        # Add the current graph's edges to the combined list\n",
    "        edge_indices.append(edges)\n",
    "\n",
    "        # Create the batch mapping for the current graph\n",
    "        batch_mapping.extend([idx] * num_nodes)\n",
    "\n",
    "        # Update the node index offset\n",
    "        current_node_index += num_nodes\n",
    "\n",
    "    # Convert the merged node list to a tensor\n",
    "    merged_nodes_tensor = torch.cat(merged_nodes, dim=0)\n",
    "\n",
    "    # Convert the edge index lists to a single tensor\n",
    "    merged_edge_indices_tensor = torch.cat(edge_indices, dim=1)\n",
    "\n",
    "    # Convert the batch mapping to a tensor\n",
    "    batch_mapping_tensor = torch.tensor(batch_mapping, dtype=torch.long)\n",
    "\n",
    "    # Convert x_water_tensor, x_amine_tensor, y_water_tensor, y_amine_tensor, temperature_tensor and P_tot_tensor to single tensors.\n",
    "    x_water_tensor = torch.stack(x_water_tensor)\n",
    "    x_amine_tensor = torch.stack(x_amine_tensor)\n",
    "    y_water_tensor = torch.stack(y_water_tensor)\n",
    "    y_amine_tensor = torch.stack(y_amine_tensor)\n",
    "    temperature_tensor = torch.stack(temperatures)\n",
    "    P_tot_tensor = torch.stack(P_tot)\n",
    "\n",
    "    return (merged_nodes_tensor, merged_edge_indices_tensor, batch_mapping_tensor), x_water_tensor, x_amine_tensor, y_water_tensor, y_amine_tensor, temperature_tensor, P_tot_tensor\n",
    "\n",
    "\n",
    "class MolecularDatasetVLE(Dataset):\n",
    "    \"\"\"\n",
    "    This class is needed to create our dataset (on the Dataset format).\n",
    "    The class inherits from the Dataset class. Input is on format X and T, where X is the SMILES notation and T is the\n",
    "    temperature. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, data):\n",
    "        # Initializes the features and targets. Is our constructor.\n",
    "        self.SMILES = data[0]\n",
    "        self.x_water = data[1]\n",
    "        self.x_amine = data[2]\n",
    "        self.y_water = data[3]\n",
    "        self.y_amine = data[4]\n",
    "        self.temperature = data[5]\n",
    "        self.P_tot = data[6]\n",
    "\n",
    "    def __len__(self):\n",
    "        # Returns the length of the dataset\n",
    "        return len(self.SMILES)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Extract the SMILES value of the molecule, calculate the graphs based on that SMILE, and then return this\n",
    "        # value along with the corresponding target value.\n",
    "        SMILES_one_molecule = self.SMILES[idx]\n",
    "        nodes, adj = smiles2graph(SMILES_one_molecule)\n",
    "\n",
    "        # Convert nodes and adj to tensors, assuming they are NumPy arrays returned from smiles2graph\n",
    "        nodes_tensor = torch.tensor(nodes, dtype=torch.float32)\n",
    "        adj_tensor = torch.tensor(adj, dtype=torch.float32)\n",
    "\n",
    "        # Convert x_water, x_amine, y_water, y_amine, temperature and pressure to tensors.\n",
    "        x_water_tensor = torch.tensor(self.x_water[idx], dtype=torch.float32)\n",
    "        x_amine_tensor = torch.tensor(self.x_amine[idx], dtype=torch.float32)\n",
    "        y_water_tensor = torch.tensor(self.y_water[idx], dtype=torch.float32)\n",
    "        y_amine_tensor = torch.tensor(self.y_amine[idx], dtype=torch.float32)\n",
    "        temperature_tensor = torch.tensor(self.temperature[idx], dtype=torch.float32)\n",
    "        P_tot_tensor = torch.tensor(self.P_tot[idx], dtype=torch.float32)\n",
    "\n",
    "        return (nodes_tensor, adj_tensor), x_water_tensor, x_amine_tensor, y_water_tensor, y_amine_tensor, temperature_tensor, P_tot_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Saturation Pressure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_general(SMILES, temperature):\n",
    "\n",
    "    # Initilize the model with the best hyperparameters.\n",
    "    model = GNNModel(num_of_features, best_config[\"num_gcn_layers\"], best_config[\"num_hidden_layers\"], best_config[\"hidden_neurons_per_layer\"], best_config[\"GCN_output_per_layer\"], best_config[\"dropout_rate\"], best_config[\"activation_function\"])\n",
    "\n",
    "    # Load the best model.\n",
    "    state_dict = torch.load(f\"Data/Best general model/Best_model.pt\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval() \n",
    "\n",
    "    # Create a dummy dataset for the y_values (will only be used for converting the SMILES etc to correct format for model)\n",
    "    y_dev = [1]*len(SMILES)\n",
    "\n",
    "    # Need to extract the mean and standard deviation for scaling. \n",
    "    mean, std = extract_mean_std(best_config[\"number_of_temp\"], best_config[\"min_interval\"])\n",
    "\n",
    "    # Convert dataset to graphs etc. \n",
    "    converted_dataset = MolecularDataset(SMILES, y_dev, temperature)\n",
    "\n",
    "    # Creating a DataLoader. \n",
    "    loader = DataLoader(converted_dataset, batch_size=1, collate_fn=merge_batch, shuffle=False)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        prediceted_pressure_values = []\n",
    "        A_list = []\n",
    "        B_list = []\n",
    "        C_list = []\n",
    "        for batch in loader:\n",
    "\n",
    "            (nodes, edge_indices, batch_mapping), targets, temperature = batch\n",
    "\n",
    "            # Calculate the predictions. \n",
    "            y_hat, A, B, C = model(nodes, edge_indices, batch_mapping, temperature, mean, std)\n",
    "\n",
    "            # Scale the output back to the original scale.\n",
    "            y_hat = y_hat * std + mean\n",
    "            \n",
    "            # Add the predicted value to the list.\n",
    "            prediceted_pressure_values.append(y_hat.item())\n",
    "            A_list.append(A.item())\n",
    "            B_list.append(B.item())\n",
    "            C_list.append(C.item())\n",
    "\n",
    "    return prediceted_pressure_values, A_list, B_list, C_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The saturation pressure of C#CCCC#C at 40 C is 2.174471378326416 mmHg\n",
      "The Antoine coefficients are A = 3.6643755435943604, B = 76.98932647705078 and C = 11.674010276794434\n"
     ]
    }
   ],
   "source": [
    "SMILES = \"C#CCCC#C\"\n",
    "temperature = 40             # Should be given in C\n",
    "\n",
    "saturation_pressure, A_pred, B_pred, C_pred = model_prediction_general([SMILES], [temperature])\n",
    "\n",
    "print(f\"The saturation pressure of {SMILES} at {temperature} C is {saturation_pressure[0]} mmHg\")\n",
    "print(f\"The Antoine coefficients are A = {A_pred[0]}, B = {B_pred[0]} and C = {C_pred[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amine Saturation Pressure Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_prediction_amine(SMILES, temperature):\n",
    "\n",
    "    model = GNNModel(num_of_features, best_config[\"num_gcn_layers\"], best_config[\"num_hidden_layers\"], best_config[\"hidden_neurons_per_layer\"], best_config[\"GCN_output_per_layer\"], best_config[\"dropout_rate\"], best_config[\"activation_function\"])\n",
    "\n",
    "    # Load the best model.\n",
    "    state_dict = torch.load(f\"Data/Best amine model/Best_model.pt\")\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval() \n",
    "\n",
    "    # Create a dummy dataset for the y_values (will only be used for converting the SMILES etc to correct format for model)\n",
    "    y_dev = [1]*len(SMILES)\n",
    "\n",
    "    # Need to extract the mean and standard deviation for scaling. \n",
    "    mean, std = extract_mean_std(best_config[\"number_of_temp\"], best_config[\"min_interval\"])\n",
    "\n",
    "    # Convert dataset to graphs etc. \n",
    "    converted_dataset = MolecularDataset(SMILES, y_dev, temperature)\n",
    "\n",
    "    # Creating a DataLoader. \n",
    "    loader = DataLoader(converted_dataset, batch_size=1, collate_fn=merge_batch, shuffle=False)\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        prediceted_pressure_values = []\n",
    "        A_list = []\n",
    "        B_list = []\n",
    "        C_list = []\n",
    "        for batch in loader:\n",
    "\n",
    "            (nodes, edge_indices, batch_mapping), targets, temperature = batch\n",
    "\n",
    "            # Calculate the predictions. \n",
    "            y_hat, A, B, C = model(nodes, edge_indices, batch_mapping, temperature, mean, std)\n",
    "\n",
    "            # Scale the output back to the original scale.\n",
    "            y_hat = y_hat * std + mean\n",
    "            \n",
    "            # Add the predicted value to the list.\n",
    "            prediceted_pressure_values.append(y_hat.item())\n",
    "            A_list.append(A.item())\n",
    "            B_list.append(B.item())\n",
    "            C_list.append(C.item())\n",
    "\n",
    "    return prediceted_pressure_values, A_list, B_list, C_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The saturation pressure of C(CCN)CN at 40 C is 0.6853728294372559 mmHg\n",
      "The Antoine coefficients are A = 3.964467763900757, B = 282.623291015625 and C = 46.18942642211914\n"
     ]
    }
   ],
   "source": [
    "SMILES = \"C(CCN)CN\"\n",
    "temperature = 40             # Should be given in C\n",
    "\n",
    "saturation_pressure, A_pred, B_pred, C_pred = model_prediction_amine([SMILES], [temperature])\n",
    "\n",
    "print(f\"The saturation pressure of {SMILES} at {temperature} C is {saturation_pressure[0]} mmHg\")\n",
    "print(f\"The Antoine coefficients are A = {A_pred[0]}, B = {B_pred[0]} and C = {C_pred[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VLE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VLE_prediction(SMILES, temperature, x_water, x_amine, P_tot):\n",
    "\n",
    "    # Extracts the mean and standard deviation for scaling. We use the same scaling as before. \n",
    "    mean_gcn, std_gcn = extract_mean_std(best_config[\"number_of_temp\"], best_config[\"min_interval\"])\n",
    "\n",
    "    # Create a dummy dataset for the y_values (will only be used for converting the SMILES etc to correct format for model)\n",
    "    y_water = [1]*len(SMILES)\n",
    "    y_amine = [1]*len(SMILES)\n",
    "    data = [[SMILES], [x_water], [x_amine], y_water, y_amine, [temperature], [P_tot]]\n",
    "\n",
    "    # Convert SMILES to graphs etc.\n",
    "    converted_dataset = MolecularDatasetVLE(data)\n",
    "\n",
    "    # Creating a DataLoader.\n",
    "    loader = DataLoader(converted_dataset, batch_size=1, collate_fn=merge_batch_VLE, shuffle=False)\n",
    "\n",
    "    # Initilize the model with the best hyperparameters. # Initilize and load the best gcn model for predicting amine pressure from earlier training.\n",
    "    gcn_saturation = GNNModel(num_of_features, best_config[\"num_gcn_layers\"], best_config[\"num_hidden_layers\"], best_config[\"hidden_neurons_per_layer\"], best_config[\"GCN_output_per_layer\"], best_config[\"dropout_rate\"], best_config[\"activation_function\"])\n",
    "\n",
    "    # Initilize the new model that will be trained.\n",
    "    model = VLE_model(gcn_saturation, mean_gcn, std_gcn, best_config, num_of_features)\n",
    "\n",
    "    # Now want to evaluate how this model performs on the training data. \n",
    "    # Load the already existing model \n",
    "    model.load_state_dict(torch.load(f\"Data/Best combined model/Best_model.pt\"))\n",
    "\n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for index, batch in enumerate(loader):\n",
    "            # Calculate the predictions for the given batch.\n",
    "            y_hat = model(batch)\n",
    "\n",
    "            return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln_y_amine:  -1.7621064186096191\n",
      "ln_y_water:  -0.12092828750610352\n"
     ]
    }
   ],
   "source": [
    "SMILES = \"CN1CCNCC1\"\n",
    "x_water = 0.5414\n",
    "x_amine = 1 - x_water\n",
    "temperature = 115.05             # Should be given in C\n",
    "P_tot = 101300                   # Should be given in Pa\n",
    "prediction = VLE_prediction(SMILES, temperature, x_water, x_amine, P_tot)\n",
    "\n",
    "ln_y_amine, ln_y_water = prediction[0][0].item(), prediction[0][1].item()\n",
    "\n",
    "print(\"ln_y_amine: \", ln_y_amine)\n",
    "print(\"ln_y_water: \", ln_y_water)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master_thesis_environment_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
